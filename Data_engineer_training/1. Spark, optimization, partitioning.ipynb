{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce354708-a2ae-42f0-b30f-f3060e387ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source distributed computing system used for big data processing and analytics. It’s known for its speed, ease of use, and ability to process large-scale data.\n",
    "\n",
    "### Key Concepts:\n",
    "**Distributed computing:** Spark processes data in parallel across many machines.\n",
    "\n",
    "**In-memory processing:** Unlike Hadoop, Spark can keep data in memory for faster access.\n",
    "\n",
    "**Unified engine:** Supports multiple tasks like batch processing, streaming, machine learning, and SQL.\n",
    "\n",
    "### Spark Ecosystem Components:\n",
    "\n",
    "| **Component**     | **Description** |\n",
    "|-------------------|-----------------|\n",
    "| **Spark SQL**     | Run SQL queries on large datasets. Supports structured and semi-structured data. |\n",
    "| **Spark Core**    | The foundation – handles basic I/O, scheduling, and task execution. |\n",
    "| **Spark Streaming** | For processing real-time data streams. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc9b963-8c60-4dff-9a9f-65194e03755f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Parallelism\n",
    "### 🍬 Real-Life Analogy: Counting M&M Lentils with Spark\n",
    "\n",
    "Imagine you're organizing a big event and you receive a **huge bag of mixed M&M lentils**—millions of them.\n",
    "\n",
    "Your goal? 👉 Count how many M&Ms there are of each color.\n",
    "\n",
    "You don't want to do this alone—it would take forever. So, you call your friends (Spark Cluster) to help you out!\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 The Spark Cluster Analogy\n",
    "\n",
    "| **Spark Component** | **Real-Life Equivalent** |\n",
    "|---------------------|--------------------------|\n",
    "| **Driver Program**  | You (the event organizer giving instructions) |\n",
    "| **Executors**       | Your friends helping count M&Ms |\n",
    "| **Cluster Manager** | Your group chat deciding who does what |\n",
    "| **Tasks**           | \"Count the red ones from this bucket\" |\n",
    "| **Partitions**      | Separate bowls of M&Ms to divide the work |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠️ How the Work is Distributed\n",
    "\n",
    "1. **Driver Program (You)**:  \n",
    "   You write down a plan: \"Count how many red, blue, green, yellow M&Ms there are.\"\n",
    "\n",
    "2. **Divide the Work (Partition the Data)**:  \n",
    "   You pour the big bag into 4 bowls (data partitions), one for each friend.\n",
    "\n",
    "3. **Send Tasks to Executors**:  \n",
    "   You give each friend a simple task:  \n",
    "   \"Count how many of each color you find in your bowl.\"\n",
    "\n",
    "4. **Friends Start Counting (Parallel Processing)**:  \n",
    "   All friends count at the same time – no waiting!  \n",
    "   Each friend returns a small result like:  \n",
    "   - Friend 1: Red – 105, Blue – 87, Green – 94  \n",
    "   - Friend 2: Red – 98, Blue – 102, Green – 101  \n",
    "   *(...and so on)*\n",
    "\n",
    "5. **Combine the Results (Aggregation at Driver)**:  \n",
    "   You collect all the partial counts and add them together:  \n",
    "   - Total Red = 105 + 98 + ...  \n",
    "   - Total Blue = 87 + 102 + ...\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Spark Benefits Shown Here:\n",
    "\n",
    "- **Speed**: Counting is done in parallel.\n",
    "- **Scalability**: You can invite more friends (add executors) if the bag gets bigger.\n",
    "- **Efficiency**: You only do the final summation (aggregation).\n",
    "\n",
    "---\n",
    "\n",
    "#### 📌 In Real Spark SQL\n",
    "\n",
    "You might run:\n",
    "```sql\n",
    "SELECT color, COUNT(*) FROM m_and_ms GROUP BY color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27b774f-3c12-4ddb-8c6e-38575d7281ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Partitioning (Legacy) & Liquid Clustering (Modern)\n",
    "---\n",
    "So how can we help with efficiency and scalability within code?  \n",
    "Traditionally, the first answer was **proper table partitioning**.  \n",
    "\n",
    "### 🧩 Table Partitioning: Distributing Work Evenly\n",
    "\n",
    "Continuing our M&M analogy 🍬...\n",
    "\n",
    "Let’s say you again have 4 friends helping to count M&Ms. But this time, you accidentally pour 90% of the M&Ms into **one** bowl, and only small amounts into the other 3.\n",
    "\n",
    "Now what happens?\n",
    "\n",
    "- One friend is **overwhelmed** with M&Ms. 😩  \n",
    "- The other three finish quickly and **sit idle**. 🪑  \n",
    "- The final result takes much longer than necessary. 🕒  \n",
    "\n",
    "This is called **data skew** — when some partitions have much more data than others.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 What is Table Partitioning?\n",
    "\n",
    "In Spark (and Delta Lake / Parquet), **partitioning** means physically organizing data files **by column values**, typically in the file system (e.g., `/table/date=2025-07-18/`).  \n",
    "\n",
    "Partitioning allows Spark to:\n",
    "\n",
    "- **Prune unnecessary data** (read only what’s needed)  \n",
    "- **Parallelize processing efficiently**  \n",
    "- **Avoid scanning full tables**  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Good Partitioning Example\n",
    "\n",
    "If you’re querying a table by **`event_date`** most of the time, you could partition by `event_date`.\n",
    "\n",
    "That way, when running:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM events WHERE event_date = '2025-07-18'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eeaf802-168a-4b01-8b24-205de834b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ⚠️ What Happens with Bad Partitioning?\n",
    "\n",
    "Let’s explore **two common partitioning mistakes**:\n",
    "\n",
    "---\n",
    "\n",
    "#### ❌ 1. Partitioning by a Skewed Column\n",
    "\n",
    "Let’s say you partition by `user_id`, and one user (e.g., `user_42`) has 90% of the activity.\n",
    "\n",
    "- Spark writes most of the data into one partition (`user_id=42`).  \n",
    "- That partition takes much longer to process.  \n",
    "- Other partitions are idle, causing **data skew** and **slow jobs**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ❌ 2. Partitioning by a Unique Key (e.g., Primary Key)\n",
    "\n",
    "Let’s say you partition by `transaction_id` or `order_id` — which are **unique for each row**.\n",
    "\n",
    "- Spark creates **one folder per row** (partition).  \n",
    "- You end up with **millions of small files** and folders.  \n",
    "- Metadata operations become slow (e.g., listing files, updating table state).  \n",
    "- Queries are slower because Spark must scan lots of tiny files.  \n",
    "- Writes can fail or severely degrade due to **filesystem overhead**.  \n",
    "\n",
    "📉 This is called the **small file problem** — and it kills performance at scale.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Key Insight\n",
    "\n",
    "> **Just because a column is important (like a primary key) doesn’t mean it should be used for partitioning.**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Better Alternatives (Legacy Approach)\n",
    "\n",
    "| Instead of this...                | Try this...                           |\n",
    "|----------------------------------|---------------------------------------|\n",
    "| Partitioning by `transaction_id` | Don’t partition, or use `event_date`  |\n",
    "| Partitioning by `user_id`        | Use `country`, `region`, or `user_type` if more balanced |\n",
    "| No partitioning at all           | Partition by columns in common filters (e.g., `event_date`, `category`) |\n",
    "\n",
    "---\n",
    "\n",
    "# 🔹 How Liquid Clustering Helps\n",
    "---\n",
    "\n",
    "With **Liquid Clustering**, these pitfalls are much less of a problem:  \n",
    "\n",
    "- Clustering doesn’t create **one folder per value** — data stays in large Delta files, reorganized internally.  \n",
    "- Skewed values (like a hot `user_id`) are handled more gracefully by **incremental reclustering**.  \n",
    "- No more **partition explosion** from unique keys — you can safely include higher-cardinality columns (e.g., `user_id`) in the clustering keys.  \n",
    "- You can **change clustering keys later** if query patterns evolve, without rewriting the whole dataset.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Modern Key Insight\n",
    "\n",
    "> **Partitioning needs careful design.  \n",
    "> Clustering gives you flexibility.**  \n",
    "\n",
    "For new Delta tables:  \n",
    "- Avoid over-partitioning.  \n",
    "- Use **`CLUSTER BY`** for most scenarios.  \n",
    "- Let **Automatic Clustering** pick keys if you don’t want to. 🤖  \n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Summary\n",
    "\n",
    "- Partitioning helps **divide the work**, but poor partitioning creates more problems than it solves.  \n",
    "- **Liquid Clustering** reduces the risk of skew and small-file issues while still giving you efficient query pruning.  \n",
    "- Spark loves **“just right” data layout** — today, that usually means **clustering** instead of heavy manual partitioning.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b7b871-7e5e-40fa-a5c5-fe1ed783d3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 Exercise 1: Partitioning vs Clustering\n",
    "\n",
    "In this exercise we’ll see **why partitioning choice matters**, and how **Liquid Clustering** provides a modern alternative.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 Steps:\n",
    "1. Create a sample dataframe / temporary view.  \n",
    "2. Create 2 **partitioned tables**:  \n",
    "   - ❌ Partitioned by a primary key (bad practice)  \n",
    "   - ✅ Partitioned by `event_date` (better practice)  \n",
    "3. Create 1 **liquid clustered table**:  \n",
    "   - 🚀 Clustered by (`event_date`)  \n",
    "4. Check table definitions.  \n",
    "5. Clean up resources.  \n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Expected Outcome\n",
    "\n",
    "- The table **partitioned by primary key** will:  \n",
    "  - Generate many tiny files/folders.  \n",
    "  - Write slowly and create metadata overhead.  \n",
    "\n",
    "- The table **partitioned by `event_date`** will:  \n",
    "  - Write more efficiently.  \n",
    "  - Allow pruning by date filters.  \n",
    "\n",
    "- The table with **Liquid Clustering** will:  \n",
    "  - Avoid the small-file problem.  \n",
    "  - Support flexible future query patterns.  \n",
    "  - Perform well even with higher-cardinality clustering keys.  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ Modern Key Insight\n",
    "\n",
    "> Partitioning can be good when used carefully, but **Liquid Clustering is the recommended default** for new Delta tables.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee6f3c4-833f-491e-a73e-7421e9d69a1e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create training dataframe"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import expr\n",
    "from datetime import datetime\n",
    "\n",
    "# Create demo DataFrame\n",
    "data = spark.range(0, 10000).selectExpr(\n",
    "    \"id as order_id\",\n",
    "    \"CAST(id % 50 AS STRING) AS user_id\",  # 50 users\n",
    "    \"date_sub(current_date(), CAST(id % 10 AS INT)) AS event_date\",\n",
    "    \"CAST(rand() * 100 AS DECIMAL(10,2)) AS amount\"\n",
    ")\n",
    "\n",
    "data.createOrReplaceTempView(\"tmp_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c04b2f-69e7-4a3d-b001-fd8606a6129b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create badly partitioned table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE TABLE exercise1_training_table_bad_partitioning \n",
    "USING DELTA\n",
    "PARTITIONED BY (order_id) -- partitioned by primary Key ( big problem)\n",
    "AS \n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  tmp_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a36b63e-dd5b-4a3d-8d44-3a8a5ee0939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE TABLE exercise1_training_table_bad_partitioning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80946c82-4a10-44a7-b86c-942252fd7f9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create well partitioned table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE exercise1_training_table_good_partitioning\n",
    "USING DELTA\n",
    "PARTITIONED BY (event_date) -- partitioned by event_date (well distributed across workers, alows to be pruned when filtered by where statement)\n",
    "AS\n",
    "SELECT * FROM tmp_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41a1fe5-8103-4e71-a9dd-f4b362184495",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":246},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752834291889}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE table exercise1_training_table_good_partitioning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1151214-1446-44c8-9482-080558544199",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Modern Alternative: Liquid Clustering"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE exercise1_training_table_liquid_clustering\n",
    "USING DELTA\n",
    "CLUSTER BY (event_date)  -- modern approach\n",
    "AS\n",
    "SELECT * FROM tmp_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2ea3a6-3051-4b45-9e3e-0a69c6b58bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE DETAIL exercise1_training_table_liquid_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2290e7d1-cd26-40c7-b945-db59ca29df65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clear objects"
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS exercise1_training_table_good_partitioning;\n",
    "DROP TABLE IF EXISTS exercise1_training_table_bad_partitioning;\n",
    "DROP TABLE IF EXISTS exercise1_training_table_liquid_clustering;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3767aeca-e103-40fa-80c3-dfe21ce67dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🧠 What Do We See? \n",
    "\n",
    "- The **well-partitioned table** (by `event_date`) took significantly **less time to create and query** than the poorly partitioned table.  \n",
    "- The **number of files** written to storage tells the story:  \n",
    "  - ❌ Bad partitioned table (by primary key, e.g., `order_id`):  \n",
    "    - 10,000 rows → 10,000 partitions → 10,000 Parquet files  \n",
    "    - Every query must scan/load all those tiny files → slow performance, high metadata overhead  \n",
    "  - ✅ Well-partitioned table (by `event_date`):  \n",
    "    - 10,000 rows → ~10 partitions → fewer, larger files → efficient reads  \n",
    "\n",
    "- 🚀 **Liquid Clustering** table:  \n",
    "  - Still creates **a manageable number of files**, but organizes data **internally by clustering key** (`event_date`)  \n",
    "  - Avoids small-file explosion even with higher cardinality columns  \n",
    "  - Query pruning works similarly to partitions but with **more flexibility**  \n",
    "\n",
    "- 📝 Key takeaway:  \n",
    "  - **Bad partitioning = many tiny files = slow queries**  \n",
    "  - **Good partitioning = fewer, balanced files = faster queries**  \n",
    "  - **Liquid clustering = modern approach**: flexible, efficient, less maintenance overhead  \n",
    "\n",
    "> In short: Spark works best when the data layout is “just right” — not too many partitions, not too few, and ideally clustered for common query patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ec9585-81df-4b0d-a4c8-2dcde9fa74d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔹 BONUS \n",
    "\n",
    "---\n",
    "\n",
    "# 🚀 Delta Optimization & Z-Ordering in Databricks vs Liquid clustering \n",
    "\n",
    "After partitioning your tables, there's more you can do to **improve performance**, especially for:\n",
    "- **Large tables**\n",
    "- **Frequent queries on specific columns**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 What is `OPTIMIZE`?\n",
    "\n",
    "Delta Lake tables can accumulate many small files over time (especially from writes or partitions).\n",
    "\n",
    "The `OPTIMIZE` command **compacts small files into larger ones**, which:\n",
    "- Reduces metadata overhead\n",
    "- Speeds up reads and filters\n",
    "- Improves shuffle performance\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 What is `ZORDER`?\n",
    "\n",
    "Z-Ordering is like sorting — but for multiple dimensions.  \n",
    "It **co-locates related data** in files, even within the same partition.\n",
    "\n",
    "> Think of ZORDER like **clustering**: putting similar records close together on disk.\n",
    "\n",
    "You should ZORDER on **columns used in WHERE filters or joins**.\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "OPTIMIZE my_table ZORDER BY (user_id, event_date);\n",
    "```\n",
    "---\n",
    "### 💧 What is Liquid Clustering (`CLUSTER BY AUTO`)?\n",
    "\n",
    "Liquid Clustering is **automatic clustering** introduced in Delta Lake to reduce manual ZORDER maintenance.  \n",
    "\n",
    "It:\n",
    "- Dynamically **reorganizes data** based on query patterns\n",
    "- Uses **column statistics and query workload** to cluster data automatically\n",
    "- Works well for **columns that frequently appear in filters or joins**, without explicitly specifying them\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "CLUSTER BY AUTO\n",
    "```\n",
    "\n",
    "---\n",
    "### ⚖️ ZORDER vs Liquid Clustering\n",
    "\n",
    "| Feature                     | ZORDER                         | Liquid Clustering (AUTO)            |\n",
    "|-------------------------------|--------------------------------|------------------------------------|\n",
    "| Manual vs Automatic          | Manual selection of columns    | Automatic selection based on queries |\n",
    "| Use case                     | Known filter/join columns      | Unknown or evolving query patterns |\n",
    "| Maintenance                  | Must update ZORDER periodically | Self-maintaining                   |\n",
    "| Performance                  | Excellent if columns chosen well | Often comparable or better over time |\n",
    "| Complexity                   | Higher (requires tuning)       | Lower (hands-off)                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3188691e-4657-4948-bf46-bf4e381205d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Spark, optimization, partitioning",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
